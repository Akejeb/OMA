# Clustering {#clustering}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

Clustering is an unsupervised machine learning technique. The idea of it is to find
clusters from the data. A cluster is a group of features/samples that share pattern.
For example, with clustering, we can find group of samples that share similar 
community composition. 

There are multiple clustering algorithms available. In next examples, we go through
hiearchical clustering and k-means clustering.

## Hiearchical clustering

Hiearchical clustering is a clustering method that aims to find hiearchy between 
samples/features. There are to approaches: agglomerative ("bottom-up") 
and divisive ("top-down"). 

In agglomerative approach, each observation is first unique cluster. 
Algorithm continues by agglomerating similar clusters. Divisive approach starts 
with one cluster that contains all the observations. Clusters are splitted recursively
to clusters that differ the most. Clustering ends when each cluster contains only one
observation.

Hiearchical clustering can be visualized with dendrogram tree. In each splitting
point, the three is divided into two clusters leading to hierarchy. 

Let's load data from mia package.

```{r hclust1}
library(mia)
library(vegan)

# Load experimental data
data(peerj13075)
(tse <- peerj13075)
```

Hierarchical clustering requires 2 steps. In the fist step, dissimilarities are 
calculated. In prior to that, data transformation is applied if needed. Since
sequencing data is compositional, relative transformation is applied.
In the second step, clustering is performed based on dissimilarities. 

```{r hclust2}
# Apply transformation
tse <- transformSamples(tse, method = "relabundance")
# Get the assay
assay <- assay(tse, "relabundance")
# Transpose assay --> samples are now in rows --> we are clustering samples
assay <- t(assay)

# Calculate distances
diss <- vegdist(assay, method = "bray")

# Perform hierarchical clustering
hc <- hclust(diss, method = "complete")

# To visualize, convert hclust object into dendrogram object
dendro <- as.dendrogram(hc)
# Plot dendrogram
plot(dendro)
```
We can use dendrogram to determine the number of clusters. Usually, the tree is splitted 
where the length of branches are the longest.

However, as we can see from the dendrogram, clusters are no clear. Let's use an algorithm to 
solve the best number of clusters.

```{r hclust3}
if( !require(NbClust) ){
    install.packages("NbClust")
    library(NbClust)
}
# Determine the optimal number of clusters
res <- NbClust(diss = diss, distance = NULL, method = "ward.D2",
               index = "silhouette")

res$Best.nc
```

Based on the result, let's divide observations into 15 clusters.

```{r hclust4}
if( !require(dendextend) ){
    install.packages("dendextend")
    library(dendextend)
}

# Find clusters
cutree(hc, k = 15) 

# Making colors for 6 clusters
col_val_map <- randomcoloR::distinctColorPalette(15) %>%
     as.list() %>% setNames(paste0("clust_",seq(15)))

dend <- color_branches(dendro, k=15, col=unlist(col_val_map))
labels(dend) <- NULL
plot(dend)

```

## K-means clustering

Because, we were not able to find clusters with hierarchical clustering, let's try
k-means clustering. In k-means clustering, observations are divided into clusters 
so that the mean distances between observations and cluster centers are minimized;
an observation belongs to cluster whose center is the nearest.

The algorithm starts by dividing observation to random clusters whose number is 
defined by user. The centroids of clusters are then calculated. After that, observations'
allocation to clusters are updated so that the means are minimized. Again, centroid 
are calculated, and algorithm continues iteratively until the assignments do not change.

The number of clusters can be determined based on algorithm. Here we utilize silhouette
analysis.

```{r kmeans1}
if( !require(factoextra) ){
    install.packages("factoextra")
    library(factoextra)
}

# Convert dist object into matrix
diss <- as.matrix(diss)
# Perform silhouette analysis and plot the result
fviz_nbclust(diss, kmeans, method = "silhouette")
```

Based on the result of silhouette analysis, we choose 3 to be the number of clusters
in k-means clustering.

```{r kmeans2}
library(scater)

# The first step is random, add seed for reproducibility
set.seed(15463)
# Perform k-means clustering with 3 clusters
km <- kmeans(diss, 3, nstart = 25)
# Add the result to colData
colData(tse)$clusters <- as.factor(km$cluster)

# Perform PCoA so that we can visualize clusters
tse <- runMDS(tse, exprs_values = "relabundance", FUN = vegan::vegdist, method = "bray")

# Plot PCoA and color clusters
plotReducedDim(tse, "MDS", colour_by = "clusters")
```

## Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```
